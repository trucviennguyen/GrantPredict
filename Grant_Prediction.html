<!DOCTYPE html>
<html>
<head>
  <title>Grant Prediction</title>
  <meta charset="utf-8">
  <meta name="description" content="Grant Prediction">
  <meta name="author" content="Vien Nguyen and Daria Eichner">
  <meta name="generator" content="slidify" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/default.css" media="all" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/phone.css" 
    media="only screen and (max-device-width: 480px)" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/slidify.css" >
  <link rel="stylesheet" href="libraries/highlighters/highlight.js/css/tomorrow.css" />
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->  
  
  <!-- Grab CDN jQuery, fall back to local if offline -->
  <script src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.7.min.js"></script>
  <script>window.jQuery || document.write('<script src="libraries/widgets/quiz/js/jquery.js"><\/script>')</script> 
  <script data-main="libraries/frameworks/io2012/js/slides" 
    src="libraries/frameworks/io2012/js/require-1.0.8.min.js">
  </script>
  
  

</head>
<body style="opacity: 0">
  <slides class="layout-widescreen">
    
    <!-- LOGO SLIDE -->
        <slide class="title-slide segue nobackground">
  <hgroup class="auto-fadein">
    <h1>Grant Prediction</h1>
    <h2></h2>
    <p>Vien Nguyen and Daria Eichner<br/></p>
  </hgroup>
  <article></article>  
</slide>
    

    <!-- SLIDES -->
    <slide class="" id="slide-1" style="background:;">
  <hgroup>
    <h2>1. Data Pre-processing</h2>
  </hgroup>
  <article data-timings="">
    <p>We work with the dataset from the University of Melbourne for grant applications. The data consists of 249 features. The task is to predict whether the grant will be given to a candidate.</p>

<p>The features can be divided into three groups:</p>

<ol>
<li>The type of the grant, e.g. sponsors, the value of the grant; </li>
<li>The research area, e.g. the RFCD code, the socio-economic objective; </li>
<li>The team members, e.g. the level of seniority, research experience, history of grant applications.</li>
</ol>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-2" style="background:;">
  <article data-timings="">
    <pre><code class="python"># Loading the data
train = pd.read_csv(&#39;data.csv&#39;)
train.head()

# Separating training features and labels
y = train[&#39;Grant.Status&#39;]
X = train.drop(&#39;Grant.Status&#39;, axis=1)
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-3" style="background:;">
  <hgroup>
    <h2>2. Data Cleaning and Feature Engineering</h2>
  </hgroup>
  <article data-timings="">
    <p>The data cleaning consists of three steps. First, we convert categorical features into indicator features.</p>

<pre><code class="python">X[&#39;Sponsor.Code&#39;] = pd.get_dummies(X[&#39;Sponsor.Code&#39;], dummy_na=True).values.argmax(1)
</code></pre>

<p>For the start date, we splitted the date and create new features: day, month, year, weekday, season.</p>

<pre><code class="python">def c_Month(x): return x.split(&quot;/&quot;)[1]
def c_Year(x): return x.split(&quot;/&quot;)[2]
def c_Weekday(x):
    d = datetime.datetime(year = int(c_Year(x)), month = int(c_Month(x)), day = int(c_Day(x)))
    return d.weekday()
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-4" style="background:;">
  <article data-timings="">
    <p>To deal with missing values in the numerical features, we replace the NaN with the mean of the features.</p>

<pre><code class="python">imp = Imputer(missing_values=&#39;NaN&#39;, strategy=&#39;mean&#39;, axis=0)
imp.fit(X.values)
X_train = imp.transform(X_train)
X_vali = imp.transform(X_vali)
X_test = imp.transform(X_test)
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-5" style="background:;">
  <hgroup>
    <h2>3. Training and Evaluation</h2>
  </hgroup>
  <article data-timings="">
    <p>We use the accuracy as well as precision, recall, F-measure and ROC AUC.</p>

<pre><code class="python">F1 = 2*(Precision * Recall)/(Precision + Recall)
</code></pre>

<p>First, we experimented with Random Forest Classifier. Then, we experiment with other learning models: Gradient Boosting, Nearest Neighbors, Decision Tree, AdaBoost, Naive Bayes, Linear Discriminant Analysis, Quadratic Discriminant Analysis.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-6" style="background:;">
  <hgroup>
    <h3>3.1 Experiments with Random Forest Classifier</h3>
  </hgroup>
  <article data-timings="">
    <pre><code class="python">rfc = RandomForestClassifier(max_depth=200, n_estimators=1250)
rfc.fit(X_train, y_train)
print rfc.score(X_vali, y_vali)
print rfc.score(X_test, y_test)
</code></pre>

<p>Validation and test accuracy:</p>

<pre><code class="python">Validation  87.26%
Test        86.32%
</code></pre>

<p>Precision and Recall in the Confusion matrix:</p>

<table><thead>
<tr>
<th>Precision</th>
<th>Recall</th>
<th>F1</th>
</tr>
</thead><tbody>
<tr>
<td>87%</td>
<td>86%</td>
<td>86%</td>
</tr>
</tbody></table>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-7" style="background:;">
  <article data-timings="">
    <p>Receiver operating characteristic (ROC curve) of the Random Forest Classifier. The ROC AUC value is 93.76%.</p>

<p><img src="figures/rfc_roc.png" alt="figures"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-8" style="background:;">
  <hgroup>
    <h3>3.2 Experiments with Gradient Boosting Classifier</h3>
  </hgroup>
  <article data-timings="">
    <pre><code class="python">gbm = ensemble.GradientBoostingClassifier(**params)
gbm.fit(X_train, y_train)
print gbm.score(X_vali, y_vali)
print gbm.score(X_test, y_test)
</code></pre>

<p>Validation and test accuracy:</p>

<pre><code class="python">Validation  88.03%
Test        85.74%
</code></pre>

<p>Precision and Recall in the Confusion matrix:</p>

<table><thead>
<tr>
<th>Precision</th>
<th>Recall</th>
<th>F1</th>
</tr>
</thead><tbody>
<tr>
<td>0.86</td>
<td>0.86</td>
<td>0.86</td>
</tr>
</tbody></table>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-9" style="background:;">
  <article data-timings="">
    <p>Receiver operating characteristic (ROC curve) of the Gradient Boosting Classifier. The ROC AUC value is 93.92%</p>

<p><img src="figures/gbm_roc.png" alt="figures"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-10" style="background:;">
  <hgroup>
    <h3>3.3 Experiments with kernel methods</h3>
  </hgroup>
  <article data-timings="">
    <p>First, we split the features into three parts:</p>

<ol>
<li>Part 1: Sponsor Code, Grant Category Code, Contract Value Band</li>
<li>Part 2: RFCD Code, RFCD Percentage, SEO Code, SEO Percentage</li>
<li>Part 3: All the features of 15 people</li>
</ol>

<p>Then, we compute three different kernels:</p>

<pre><code class="python">K1      Linear kernel of part 1
K2      Linear kernel of part 2
K3      Linear kernel of part 3
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-11" style="background:;">
  <article data-timings="">
    <p>The combined kernel is derived by taking the average of the three kernels:</p>

<pre><code class="python">K = (K1 + K2 + K3)/3
</code></pre>

<p>Results of the combined kernel on the validation and test set:</p>

<table><thead>
<tr>
<th></th>
<th>Accuracy</th>
<th>F1</th>
</tr>
</thead><tbody>
<tr>
<td>Validation</td>
<td>67.57%</td>
<td>52.76%</td>
</tr>
<tr>
<td>Test</td>
<td>69.88%</td>
<td>55.93%</td>
</tr>
</tbody></table>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-12" style="background:;">
  <hgroup>
    <h3>3.4 Experiments with other classifiers</h3>
  </hgroup>
  <article data-timings="">
    <table><thead>
<tr>
<th>Classifier</th>
<th>Accuracy</th>
<th>F-measure</th>
</tr>
</thead><tbody>
<tr>
<td>Nearest Neighbors</td>
<td>53.95</td>
<td>55</td>
</tr>
<tr>
<td>Decision Tree</td>
<td>83.94</td>
<td>84</td>
</tr>
<tr>
<td>AdaBoost</td>
<td>82.85</td>
<td>83</td>
</tr>
<tr>
<td>Naive Bayes</td>
<td>63.46</td>
<td>50</td>
</tr>
<tr>
<td>Linear Discriminant Analysis</td>
<td>78.36</td>
<td>78</td>
</tr>
<tr>
<td>Quadratic Discriminant Analysis</td>
<td>63.26</td>
<td>50</td>
</tr>
</tbody></table>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-13" style="background:;">
  <hgroup>
    <h2>4. Conclusion</h2>
  </hgroup>
  <article data-timings="">
    <p>We established a first baseline without any feature engineering, afterwards we started several model improvments such as:</p>

<ol>
<li>Adding new features (date-&gt; Day, Month, Year) to the data.</li>
<li>Parameterization of classifiers with and without Gridsearch.</li>
<li>Employ pipeline models: First we train a model for people, called Team model, then we apply the results on the origin data.</li>
<li>Comparision of other classifiers to our Random Forest baseline score.</li>
</ol>

<p>The best results are derived with Gradient Boosting Classifier, following these steps:</p>

<ol>
<li>Convert from categorical to indicator variables.</li>
<li>Convert the date to day, month, year, weekday, and season and add to the origin data.</li>
<li>Impute missing values with the mean.</li>
<li>Train and test with Gradient Boosting Classifier.</li>
</ol>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-14" style="background:;">
  <hgroup>
    <h2>Thank you!</h2>
  </hgroup>
  <article data-timings="">
    
  </article>
  <!-- Presenter Notes -->
</slide>

    <slide class="backdrop"></slide>
  </slides>
  <div class="pagination pagination-small" id='io2012-ptoc' style="display:none;">
    <ul>
      <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=1 title='1. Data Pre-processing'>
         1
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=2 title='NA'>
         2
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=3 title='2. Data Cleaning and Feature Engineering'>
         3
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=4 title='NA'>
         4
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=5 title='3. Training and Evaluation'>
         5
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=6 title='3.1 Experiments with Random Forest Classifier'>
         6
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=7 title='NA'>
         7
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=8 title='3.2 Experiments with Gradient Boosting Classifier'>
         8
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=9 title='NA'>
         9
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=10 title='3.3 Experiments with kernel methods'>
         10
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=11 title='NA'>
         11
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=12 title='3.4 Experiments with other classifiers'>
         12
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=13 title='4. Conclusion'>
         13
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=14 title='Thank you!'>
         14
      </a>
    </li>
  </ul>
  </div>  <!--[if IE]>
    <script 
      src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js">  
    </script>
    <script>CFInstall.check({mode: 'overlay'});</script>
  <![endif]-->
</body>
  <!-- Load Javascripts for Widgets -->
  
  <!-- LOAD HIGHLIGHTER JS FILES -->
  <script src="libraries/highlighters/highlight.js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <!-- DONE LOADING HIGHLIGHTER JS FILES -->
   
  </html>